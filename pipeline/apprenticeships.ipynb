{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 'https://explore-education-statistics.service.gov.uk/find-statistics/apprenticeships'\n",
    "la_codes = ['E06000014']\n",
    "ssa_tier_1_codes = [\"Construction, Planning and the Built Environment\"]\n",
    "DATA_DIR = Path('../static/autoload-data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprenticeship data\n",
    "\n",
    "Get the latest apprenticeship data, which is available from a web page. First we need to find the proper link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://content.explore-education-statistics.service.gov.uk/api/releases/af26c28f-9d85-4130-8f4c-a1dd4f40c2c4/files'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with requests.get(page) as r:\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    zip_link = soup.find('a', string='Download all data (zip)')['href']\n",
    "zip_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download the zip file ready to extract the relevant data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.get(zip_link) as r:\n",
    "    zip = ZipFile(BytesIO(r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the detailed information which has starts apprenticeships. The raw data that we need is stored in a zip file in the `supporting-files` sub-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = next((x for x in zip.namelist() if \"supporting-files/app-underlying-data-starts\" in x))\n",
    "\n",
    "with zip.open(zip_path) as f:\n",
    "    with ZipFile(f) as sub_zip:\n",
    "        with sub_zip.open(sub_zip.namelist()[0]) as raw_csv:\n",
    "            starts_df = pd.read_csv(raw_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achievements data is in a similar location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = next((x for x in zip.namelist() if \"supporting-files/app-underlying-data-achievements\" in x))\n",
    "\n",
    "with zip.open(zip_path) as f:\n",
    "    with ZipFile(f) as sub_zip:\n",
    "        with sub_zip.open(sub_zip.namelist()[0]) as raw_csv:\n",
    "            achievements_df = pd.read_csv(raw_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some filtering and processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_date(df, new_col='year_start'):\n",
    "    df[new_col] = df.year.astype(str).str.slice(0, 4).pipe(pd.to_datetime, format=\"%Y\") + pd.DateOffset(months=8)\n",
    "    return df\n",
    "def filter_construction(df):\n",
    "    return df.loc[df.ssa_tier_1.isin(ssa_tier_1_codes)]\n",
    "def filter_york_learners(df):\n",
    "    return df.loc[df.learner_home_la_code.isin(la_codes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, sum_field):\n",
    "    return df.pipe(\n",
    "        filter_construction\n",
    "    ).groupby(\n",
    "        ['learner_home_la_code', 'year', 'apps_level']\n",
    "    )[\n",
    "        sum_field\n",
    "    ].sum(\n",
    "    ).to_frame(\n",
    "    ).melt(\n",
    "        ignore_index=False\n",
    "    )\n",
    "def relabel(df):\n",
    "    return df.reset_index(\n",
    "    ).rename(\n",
    "        columns={'learner_home_la_code': 'ons_code'}\n",
    "    ).pipe(\n",
    "        explode_date, 'year'\n",
    "    ).set_index([\n",
    "        'ons_code',\n",
    "        'year',\n",
    "        'apps_level'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two files into one and save as a partitioned Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "APPRENTICE_DATA = DATA_DIR / 'apprenticeships/'\n",
    "pd.concat(\n",
    "    [\n",
    "        starts_df.pipe(aggregate, 'starts'),\n",
    "        achievements_df.pipe(aggregate, 'achievements'),\n",
    "    ]\n",
    ").pipe(\n",
    "    relabel\n",
    ").to_parquet(\n",
    "    APPRENTICE_DATA,\n",
    "    partition_cols=['ons_code']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is already a summary by `la_code`, which we'll also extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = next((x for x in zip.namelist() if \"data/app-starts-since-\" in x))\n",
    "\n",
    "with zip.open(csv_path) as f:\n",
    "    data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = (\n",
    "  data\n",
    "    .loc[data.geographic_level == 'Local authority', :]\n",
    "    .drop(columns=['pcon_code', 'pcon_name', 'old_la_code', 'geographic_level'])\n",
    "    .rename(columns={'new_la_code': 'geo_code', 'la_name': 'geo_name'})\n",
    ")\n",
    "filtered['date'] = filtered.time_period.astype(str).str.slice(0,4).apply(pd.to_datetime) + pd.DateOffset(months=8, years=1, days=-1)\n",
    "export = (\n",
    "  filtered\n",
    "    .loc[:, ['date', 'geo_code', 'starts']]\n",
    "    .melt(id_vars=['geo_code', 'date'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is saved as a single file and partitioned, if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_parquet(DATA_DIR / 'data.parquet')\n",
    "# export.to_parquet(DATA_DIR / 'data', partition_cols=['geo_code'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-value-data-explorer-RFFikNqF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
